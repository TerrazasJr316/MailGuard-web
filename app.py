# app.py

import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay
import model_trainer # Importamos nuestro script de l√≥gica

# --- Configuraci√≥n de la P√°gina de Streamlit ---
st.set_page_config(
    page_title="Detector de SPAM con Regresi√≥n Log√≠stica",
    page_icon="üìß",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Funci√≥n para descargar datos de NLTK (se cachea para no descargar cada vez) ---
@st.cache_resource
def setup_nltk():
    """Descarga los datos necesarios de NLTK."""
    model_trainer.download_nltk_data()

# Llamar a la funci√≥n de configuraci√≥n al inicio
setup_nltk()

# --- T√≠tulo y Descripci√≥n de la Aplicaci√≥n ---
st.title("üìß Detector de SPAM con Regresi√≥n Log√≠stica")
st.markdown("""
Abstract
NSL-KDD is a data set suggested to solve some of the inherent problems of the KDD'99 data set which are mentioned in [1].
Although, this new version of the KDD data set still suffers from some of the problems discussed by McHugh [2] and may not
be a perfect representative of existing real networks, because of the lack of public data sets for network-based IDSs, we
believe it still can be applied as an effective benchmark data set to help researchers compare different intrusion detection
methods. Furthermore, the number of records in the NSL-KDD train and test sets are reasonable. This advantage makes it affordable
to run the experiments on the complete set without the need to randomly select a small portion. Consequently, evaluation results
of different research work will be consistent and comparable.

Data Files
* KDDTrain+.ARFF - The full NSL-KDD train set with binary labels in ARFF format
* KDDTrain+.TXT - The full NSL-KDD train set including attack-type labels and difficulty level in CSV format
* KDDTrain+_20Percent.ARFF - A 20% subset of the KDDTrain+.arff file
* KDDTrain+_20Percent.TXT - A 20% subset of the KDDTrain+.txt file
* KDDTest+.ARFF - The full NSL-KDD test set with binary labels in ARFF format
* KDDTest+.TXT - The full NSL-KDD test set including attack-type labels and difficulty level in CSV format
* KDDTest-21.ARFF - A subset of the KDDTest+.arff file which does not include records with difficulty level of 21 out of 21
* KDDTest-21.TXT - A subset of the KDDTest+.txt file which does not include records with difficulty level of 21 out of 21

Descargar los ficheros
https://www.unb.ca/cic/datasets/nsl.html

Referencias adicionales sobre el DataSet
References: [1] M. Tavallaee, E. Bagheri, W. Lu, and A. Ghorbani, ‚ÄúA Detailed Analysis of the KDD CUP 99 Data Set,‚Äù
Submitted to Second IEEE Symposium on Computational Intelligence for Security and Defense Applications (CISDA), 2009.

Esta aplicaci√≥n web te permite entrenar un modelo de Machine Learning para detectar correos SPAM.
Puedes elegir cu√°ntos correos usar para el entrenamiento y ver en tiempo real c√≥mo impacta en el rendimiento del modelo.
Las m√©tricas se calculan sobre un conjunto de prueba fijo de **2000 correos** que el modelo no ha visto durante el entrenamiento.
""")

# --- Barra Lateral de Opciones (Sidebar) ---
with st.sidebar:
    st.header("‚öôÔ∏è Par√°metros del Modelo")
    
    # Slider para que el usuario elija el n√∫mero de muestras de entrenamiento
    num_samples = st.slider(
        "N√∫mero de correos para entrenar:",
        min_value=1000,
        max_value=20000,
        value=10000,  # Valor por defecto que da un buen F1-Score
        step=1000,
        help="Selecciona la cantidad de datos para el entrenamiento. Un n√∫mero mayor puede mejorar la precisi√≥n pero tardar√° m√°s en procesar."
    )

    # Bot√≥n para iniciar el entrenamiento
    train_button = st.button("üöÄ Entrenar y Evaluar Modelo", type="primary", use_container_width=True)
    
    st.markdown("---")
    st.info("""
            Se propone la contrucci√≥n de aprendizaje autom√°tico capaz de predecir si un correo determinado se SPAM o no, para esto se utilizar√° el siguiente DataSet (Conjunto de Datos):

            2007 TREC Public Spam Corpurs
            
            The corpus trec07p contains 75,419 messages:

            25220 ham
            
            50199 spam
            
            These messages constitute all the messages delivered to a particular server between these dates:

            Sun, 8 Apr 2007 13:07:21 -0400
            
            Fri, 6 Jul 2007 07:04:53 -0400
            
            Esta app fue creada como demostraci√≥n a partir de cuadernos de Jupyter. El dataset utilizado es el **TREC 2007 Spam Corpus**.
            """)

# --- L√≥gica Principal de la Aplicaci√≥n ---

# Placeholder para mostrar los resultados despu√©s del entrenamiento
results_placeholder = st.empty()

if train_button:
    
    # Usar el placeholder para mostrar el proceso y los resultados
    with results_placeholder.container():
        # Mensaje explicativo sobre el conteo total de correos
        total_to_process = num_samples + 2000
        st.info(f"Iniciando proceso... Se cargar√°n y procesar√°n **{total_to_process}** correos en total ({num_samples} para entrenar y 2000 para probar).")
        
        st.header(f"Resultados de Entrenamiento con {num_samples} correos")
        
        # Usar un spinner para mostrar que el proceso est√° en marcha
        progress_bar = st.progress(0, text="Iniciando proceso...")
        status_text = st.empty()

        def update_status(message, progress):
            """Funci√≥n de callback para actualizar la UI desde el trainer."""
            status_text.text(message)
            progress_bar.progress(progress, text=message)

        try:
            # Llamar a la funci√≥n principal de nuestro m√≥dulo de entrenamiento
            results = model_trainer.train_and_evaluate(
                num_training_samples=num_samples,
                status_callback=update_status
            )
            
            # --- Mostrar M√©tricas Clave en columnas ---
            st.subheader("üìä M√©tricas de Rendimiento")
            col1, col2 = st.columns(2)
            
            # M√©trica de Accuracy
            col1.metric(
                label="‚úÖ Accuracy (Precisi√≥n Global)",
                value=f"{results['accuracy']:.3f}",
                help="Porcentaje de predicciones correctas sobre el total de correos de prueba."
            )
            
            # M√©trica de F1-Score
            f1_color = "normal" if results['f1_score'] > 0.95 else "inverse"
            col2.metric(
                label="üéØ F1-Score (SPAM)",
                value=f"{results['f1_score']:.3f}",
                help="Media arm√≥nica de Precisi√≥n y Recall. Es la m√©trica m√°s importante para SPAM, ya que balancea los falsos positivos y negativos.",
                delta_color=f1_color
            )
            
            st.markdown("---")
            
            # --- Mostrar Visualizaciones ---
            st.subheader("üìà Visualizaciones del Modelo")
            
            # Crear columnas para los gr√°ficos
            fig_col1, fig_col2 = st.columns(2)
            
            with fig_col1:
                st.markdown("#### Matriz de Confusi√≥n")
                fig, ax = plt.subplots(figsize=(6, 5))
                ConfusionMatrixDisplay(
                    confusion_matrix=results['confusion_matrix'],
                    display_labels=results['classes']
                ).plot(ax=ax, cmap='Blues', values_format='d')
                ax.set_title("Rendimiento en el set de prueba")
                st.pyplot(fig)
                st.markdown("""
                - **Verdaderos Positivos (VP)**: SPAM real, predicho como SPAM.
                - **Verdaderos Negativos (VN)**: HAM real, predicho como HAM.
                - **Falsos Positivos (FP)**: HAM real, predicho como SPAM (¬°Error grave!).
                - **Falsos Negativos (FN)**: SPAM real, predicho como HAM.
                """)
            
            with fig_col2:
                st.markdown("#### Curva ROC")
                fig_roc, ax_roc = plt.subplots(figsize=(6, 5))
                RocCurveDisplay.from_estimator(
                    results['classifier'],
                    results['X_test'],
                    results['y_test'],
                    ax=ax_roc,
                    pos_label='spam'
                )
                ax_roc.set_title("Capacidad de Discriminaci√≥n")
                st.pyplot(fig_roc)
                st.markdown("""
                Muestra la capacidad del modelo para distinguir entre clases. Un √°rea bajo la curva (AUC) cercana a 1.0 indica un excelente rendimiento.
                """)

            st.markdown("#### Curva de Precisi√≥n-Recall (PR)")
            fig_pr, ax_pr = plt.subplots(figsize=(10, 5))
            PrecisionRecallDisplay.from_estimator(
                results['classifier'],
                results['X_test'],
                results['y_test'],
                ax=ax_pr,
                pos_label='spam'
            )
            ax_pr.set_title("Balance entre Precisi√≥n y Recall")
            st.pyplot(fig_pr)
            st.markdown("""
            Esta curva es √∫til cuando las clases est√°n desbalanceadas. Muestra el balance entre la **precisi√≥n** (qu√© tan acertadas son las predicciones de SPAM) y el **recall** (cu√°ntos de los SPAM reales se detectaron).
            """)
            
            st.success("¬°El modelo fue entrenado y evaluado con √©xito!")
            
        except Exception as e:
            st.error(f"Ocurri√≥ un error durante el proceso: {e}")
            st.error("Aseg√∫rate de que la carpeta 'ALERT' con los datasets est√© en el directorio correcto y que el archivo de √≠ndice no est√© corrupto.")

else:
    # Mensaje inicial antes de presionar el bot√≥n
    with results_placeholder.container():
        st.info("Configura los par√°metros en la barra lateral y presiona 'Entrenar y Evaluar Modelo' para comenzar.")
